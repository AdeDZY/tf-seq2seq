{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.data_utils import prepare_data\n",
    "from data.data_utils import TextIterator\n",
    "from seq2seq_model import Seq2SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data loading parameters\n",
    "tf.app.flags.DEFINE_string('source_vocabulary', 'data/en-fr/wmt15_fr-en.train.en.json', 'Path to source vocabulary')\n",
    "tf.app.flags.DEFINE_string('target_vocabulary', 'data/en-fr/wmt15_fr-en.train.fr.json', 'Path to target vocabulary')\n",
    "tf.app.flags.DEFINE_string('source_train_data', 'data/en-fr/wmt15_fr-en.train.en', 'Path to source training data')\n",
    "tf.app.flags.DEFINE_string('target_train_data', 'data/en-fr/wmt15_fr-en.train.fr', 'Path to target training data')\n",
    "tf.app.flags.DEFINE_string('source_valid_data', 'data/en-fr/newstest2013.tok.en', 'Path to source validation data')\n",
    "tf.app.flags.DEFINE_string('target_valid_data', 'data/en-fr/newstest2013.tok.fr', 'Path to target validation data')\n",
    "\n",
    "# Network parameters\n",
    "tf.app.flags.DEFINE_string('cell_type', 'lstm', 'RNN cell to use for encoder and decoder')\n",
    "tf.app.flags.DEFINE_string('attention_type', 'bahdanau', 'Attention mechanism: (bahdanau, luong)')\n",
    "tf.app.flags.DEFINE_integer('hidden_units', 1024, 'Number of hidden units for each layer in the model')\n",
    "tf.app.flags.DEFINE_integer('depth', 4, 'Number of layers for each encoder and decoder')\n",
    "tf.app.flags.DEFINE_integer('embedding_size', 500, 'Embedding dimensions of encoder and decoder inputs')\n",
    "tf.app.flags.DEFINE_integer('num_encoder_symbols', 30000, 'Source vocabulary size')\n",
    "tf.app.flags.DEFINE_integer('num_decoder_symbols', 30000, 'Target vocabulary size')\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('use_residual', True, 'Use residual connection between layers')\n",
    "tf.app.flags.DEFINE_boolean('input_feeding', True, 'Use input feeding method in attentional decoder')\n",
    "tf.app.flags.DEFINE_boolean('use_dropout', True, 'Use dropout in each rnn cell')\n",
    "tf.app.flags.DEFINE_float('dropout_keep_prob', 0.3, 'Dropout keep probability for input/output/state units (1.0: no dropout)')\n",
    "\n",
    "# Training parameters\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.0002, 'Learning rate')\n",
    "tf.app.flags.DEFINE_float('max_gradient_norm', 1.0, 'Clip gradients to this norm')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128, 'Batch size to use during training')\n",
    "tf.app.flags.DEFINE_integer('max_epochs', 10, 'Maximum # of training epochs')\n",
    "tf.app.flags.DEFINE_integer('max_load_batches', 20, 'Maximum # of batches to load at one time')\n",
    "tf.app.flags.DEFINE_integer('max_seq_length', 50, 'Maximum sequence length')\n",
    "tf.app.flags.DEFINE_integer('display_freq', 100, 'Display training status every this iteration')\n",
    "tf.app.flags.DEFINE_integer('save_freq', 1000, 'Save model checkpoint every this iteration')\n",
    "tf.app.flags.DEFINE_integer('valid_freq', 1000, 'Evaluate model every this iteration: valid_data needed')\n",
    "tf.app.flags.DEFINE_string('optimizer', 'adam', 'Optimizer for training: (adadelta, adam, rmsprop)')\n",
    "tf.app.flags.DEFINE_string('model_dir', 'model/', 'Path to save model checkpoints')\n",
    "tf.app.flags.DEFINE_string('summary_dir', 'model/summary', 'Path to save model summary')\n",
    "tf.app.flags.DEFINE_string('model_name', 'translate.ckpt', 'File name used for model checkpoints')\n",
    "tf.app.flags.DEFINE_boolean('shuffle_each_epoch', True, 'Shuffle training dataset for each epoch')                            \n",
    "\n",
    "# Runtime parameters\n",
    "tf.app.flags.DEFINE_boolean('use_fp16', False, 'Use half precision float16 instead of float32 as dtype')\n",
    "tf.app.flags.DEFINE_boolean('allow_soft_placement', True, 'Allow device soft placement')\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False, 'Log placement of ops on devices')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(session, FLAGS):\n",
    "    model = Seq2SeqModel(FLAGS, 'decoding')\n",
    "    \n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.model_dir)\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        print 'Reloading model parameters..'\n",
    "        model.restore(session, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    else:\n",
    "        print 'Tensorflow checkpoints do not exist'\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
